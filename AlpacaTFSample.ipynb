{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlpacaTFSample.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8-2kOoDgvARd"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QROST/AlpacaPlayground/blob/main/AlpacaTFSample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6OGGTuYsilc"
      },
      "source": [
        "Reference: https://alpaca.markets/learn/tensorflow-market-forecasting/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-2kOoDgvARd"
      },
      "source": [
        "### **Dataset Prep**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0_WYeEIsMt8"
      },
      "source": [
        "Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t56BN2JXXn_K",
        "outputId": "58e73cb6-2bcf-4e56-9a7c-619f3405ef87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install alpaca-trade-api\n",
        "!pip install --upgrade mplfinance\n",
        "!wget https://launchpad.net/~mario-mariomedina/+archive/ubuntu/talib/+files/libta-lib0_0.4.0-oneiric1_amd64.deb -qO libta.deb\n",
        "!wget https://launchpad.net/~mario-mariomedina/+archive/ubuntu/talib/+files/ta-lib0-dev_0.4.0-oneiric1_amd64.deb -qO ta.deb\n",
        "!dpkg -i libta.deb ta.deb\n",
        "!pip install ta-lib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: alpaca-trade-api in /usr/local/lib/python3.6/dist-packages (0.51.0)\n",
            "Requirement already satisfied: urllib3<1.26,>1.24 in /usr/local/lib/python3.6/dist-packages (from alpaca-trade-api) (1.24.3)\n",
            "Requirement already satisfied: websockets<9,>=8.0 in /usr/local/lib/python3.6/dist-packages (from alpaca-trade-api) (8.1)\n",
            "Requirement already satisfied: websocket-client<1,>=0.56.0 in /usr/local/lib/python3.6/dist-packages (from alpaca-trade-api) (0.57.0)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.6/dist-packages (from alpaca-trade-api) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from alpaca-trade-api) (1.1.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client<1,>=0.56.0->alpaca-trade-api) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>2->alpaca-trade-api) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>2->alpaca-trade-api) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>2->alpaca-trade-api) (2020.6.20)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->alpaca-trade-api) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas->alpaca-trade-api) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->alpaca-trade-api) (2.8.1)\n",
            "Requirement already up-to-date: mplfinance in /usr/local/lib/python3.6/dist-packages (0.12.7a0)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from mplfinance) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from mplfinance) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mplfinance) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mplfinance) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mplfinance) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mplfinance) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mplfinance) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->mplfinance) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->mplfinance) (1.15.0)\n",
            "(Reading database ... 144654 files and directories currently installed.)\n",
            "Preparing to unpack libta.deb ...\n",
            "Unpacking libta-lib0 (0.4.0-oneiric1) over (0.4.0-oneiric1) ...\n",
            "Preparing to unpack ta.deb ...\n",
            "Unpacking ta-lib0-dev (0.4.0-oneiric1) over (0.4.0-oneiric1) ...\n",
            "Setting up libta-lib0 (0.4.0-oneiric1) ...\n",
            "Setting up ta-lib0-dev (0.4.0-oneiric1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: ta-lib in /usr/local/lib/python3.6/dist-packages (0.4.19)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ta-lib) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0ZTYhbLzFb9"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjoT5Sp6sTIl"
      },
      "source": [
        "import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPhPBaw6XZ4L"
      },
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import talib\n",
        "import alpaca_trade_api as tradeapi\n",
        "import pandas\n",
        "\n",
        "from time import sleep\n",
        "from time import time\n",
        "import datetime\n",
        "\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "import argparse\n",
        "import sys\n",
        "import tempfile\n",
        "\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tfc\n",
        "from sklearn import metrics"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-GW7gUDse9o"
      },
      "source": [
        "Initiate settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsqNbxg7scPT"
      },
      "source": [
        "# Creates dataset folders in directory script is run from\n",
        "try:\n",
        "    os.stat(\"./train\")\n",
        "    os.stat(\"./eval\")\n",
        "except BaseException:\n",
        "    os.mkdir(\"./train\")\n",
        "    os.mkdir(\"./eval\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbxW6voS1xMh"
      },
      "source": [
        "#paper trade account\n",
        "api = tradeapi.REST(\n",
        "    \"PKW1DKS7S7VY873SRVCZ\",\n",
        "    \"Xi6LgmUGtakBizsSVfb3o3os9Im4v7s0R8gOn4GO\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i533XephXlPE"
      },
      "source": [
        "barTimeframe = \"1D\"  # 1Min, 5Min, 15Min, 1H, 1D\n",
        "\n",
        "assetList = ['NVDA','MSFT','AMD','TRMB']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPAWmO_DbzPo"
      },
      "source": [
        "# ISO8601 date format\n",
        "trainStartDate = \"2000-01-01T00:00:00.000Z\"\n",
        "trainEndDate = \"2017-06-01T00:00:00.000Z\"\n",
        "evalStartDate = \"2017-06-01T00:00:00.000Z\"\n",
        "evalEndDate = \"2020-10-01T00:00:00.000Z\"\n",
        "\n",
        "targetLookaheadPeriod = 1\n",
        "startCutoffPeriod = 50  # Set to length of maximum period indicator\n",
        "\n",
        "# Tracks position in list of symbols to download\n",
        "iteratorPos = 0\n",
        "assetListLen = len(assetList)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfKBqkQeuCm3"
      },
      "source": [
        "def CreateDataSet(iteratorPos):\n",
        "    try:\n",
        "        symbol = assetList[iteratorPos]\n",
        "        # Returns market data as a pandas dataframe\n",
        "        returned_data = api.get_barset(\n",
        "            symbol,\n",
        "            barTimeframe,\n",
        "            start=trainStartDate,\n",
        "            end=evalEndDate).df\n",
        "        # Processes all data into numpy arrays for use by talib\n",
        "        timeList = np.array(returned_data.index)\n",
        "        openList = np.array(returned_data[symbol]['open'].tolist(), dtype=np.float64)\n",
        "        print(openList)\n",
        "        #print(returned_data)\n",
        "\n",
        "        highList = np.array(returned_data[symbol]['high'].tolist(), dtype=np.float64)\n",
        "        lowList = np.array(returned_data[symbol]['low'].tolist(), dtype=np.float64)\n",
        "        closeList = np.array(returned_data[symbol]['close'].tolist(), dtype=np.float64)\n",
        "        volumeList = np.array(returned_data[symbol]['volume'].tolist(), dtype=np.float64)\n",
        "        # Adjusts data lists due to the reward function look ahead period\n",
        "        shiftedTimeList = timeList[:-targetLookaheadPeriod]\n",
        "        shiftedClose = closeList[targetLookaheadPeriod:]\n",
        "        highList = highList[:-targetLookaheadPeriod]\n",
        "        lowList = lowList[:-targetLookaheadPeriod]\n",
        "        closeList = closeList[:-targetLookaheadPeriod]\n",
        "        # Calculate trading indicators\n",
        "        RSI14 = talib.RSI(closeList, 14)\n",
        "        RSI50 = talib.RSI(closeList, 50)\n",
        "        STOCH14K, STOCH14D = talib.STOCH(\n",
        "            highList, lowList, closeList, fastk_period=14, slowk_period=3, slowd_period=3)\n",
        "        # Calulate network target/ reward function for training\n",
        "        closeDifference = shiftedClose - closeList\n",
        "        closeDifferenceLen = len(closeDifference)\n",
        "        # Creates a binary output if the market moves up or down, for use as\n",
        "        # one-hot labels\n",
        "\n",
        "        longOutput = np.zeros(closeDifferenceLen)\n",
        "        longOutput[closeDifference >= 0] = 1\n",
        "        shortOutput = np.zeros(closeDifferenceLen)\n",
        "        shortOutput[closeDifference < 0] = 1\n",
        "        # Constructs the dataframe and writes to CSV file\n",
        "        outputDF = {\n",
        "            \"close\": closeList,  # Not to be included in network training, only for later analysis\n",
        "            \"RSI14\": RSI14,\n",
        "            \"RSI50\": RSI50,\n",
        "            \"STOCH14K\": STOCH14K,\n",
        "            \"STOCH14D\": STOCH14D,\n",
        "            \"longOutput\": longOutput,\n",
        "            \"shortOutput\": shortOutput\n",
        "        }\n",
        "        # Makes sure the dataframe columns don't get mixed up\n",
        "        columnOrder = [\"close\", \"RSI14\", \"RSI50\", \"STOCH14K\",\n",
        "                        \"STOCH14D\", \"longOutput\", \"shortOutput\"]\n",
        "        outputDF = pandas.DataFrame(\n",
        "            data=outputDF,\n",
        "            index=shiftedTimeList,\n",
        "            columns=columnOrder)[\n",
        "            startCutoffPeriod:]\n",
        "        # Splits data into training and evaluation sets\n",
        "        trainingDF = outputDF[outputDF.index < evalStartDate]\n",
        "        evalDF = outputDF[outputDF.index >= evalStartDate]\n",
        "\n",
        "        if (len(trainingDF) > 0 and len(evalDF) > 0):\n",
        "            print(\"writing \" + str(symbol) +\n",
        "                  \", data len: \" + str(len(closeList)))\n",
        "\n",
        "            trainingDF.to_csv(\"./train/\" + symbol + \".csv\", index_label=\"date\")\n",
        "            evalDF.to_csv(\"./eval/\" + symbol + \".csv\", index_label=\"date\")\n",
        "    except BaseException:\n",
        "        pass\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq75DnO02oZ0",
        "outputId": "3d254932-2457-4ac4-9d44-f90d8d569934",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#iterate through target stock symbols and create their data sets\n",
        "while iteratorPos < assetListLen:   \n",
        "    CreateDataSet(iteratorPos)   \n",
        "    sleep(5)  # To avoid API rate limits\n",
        "    iteratorPos += 1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 34.17  33.2   31.88 ... 521.61 517.5  526.3 ]\n",
            "writing NVDA, data len: 3209\n",
            "[ 35.79  35.24  35.22 ... 210.95 209.35 207.73]\n",
            "writing MSFT, data len: 3209\n",
            "[ 7.36    7.15    6.64   ... 79.1165 79.3    81.75  ]\n",
            "writing AMD, data len: 3209\n",
            "[15.265 14.765 14.625 ... 48.8   48.62  49.   ]\n",
            "writing TRMB, data len: 3207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1SK_K-yu3de"
      },
      "source": [
        "### **Training session**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuCgLuuvbluO"
      },
      "source": [
        "# model settings\n",
        "# Static seed to allow for reproducability between training runs\n",
        "tf.random.set_seed(12345)\n",
        "trainingCycles = 500000  # Number of training steps before ending\n",
        "batchSize = 1000  # Number of examples per training batch\n",
        "summarySteps = 1000  # Number of training steps between each summary\n",
        "dropout = 0.5  # Node dropout for training\n",
        "nodeLayout = [40, 30, 20, 10]  # Layout of nodes in each layer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuzGLqYfGx5O"
      },
      "source": [
        "mainDirectory = str(\"./model_1/\")\n",
        "\n",
        "trainFiles = [f for f in listdir(\"./train/\") if isfile(join(\"./train/\", f))]\n",
        "evalFiles = [f for f in listdir(\"./eval/\") if isfile(join(\"./eval/\", f))]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfGcQeIRHfPb"
      },
      "source": [
        "# Initialises data arrays\n",
        "trainDataX = np.empty([0, 4])\n",
        "trainDataY = np.empty([0, 2])\n",
        "evalDataX = np.empty([0, 4])\n",
        "evalDataY = np.empty([0, 2])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHsBRynIHhav",
        "outputId": "63629893-6f97-48d3-ac97-8aefe62879ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Reads training data into memory\n",
        "readPos = 0\n",
        "for fileName in trainFiles:\n",
        "    importedData = pandas.read_csv(\"./train/\" + fileName, sep=',')\n",
        "\n",
        "    xValuesDF = importedData[[\"RSI14\", \"RSI50\", \"STOCH14K\", \"STOCH14D\"]]\n",
        "    yValuesDF = importedData[[\"longOutput\", \"shortOutput\"]]\n",
        "\n",
        "    xValues = np.array(xValuesDF.values.tolist())\n",
        "    yValues = np.array(yValuesDF.values.tolist())\n",
        "\n",
        "    trainDataX = np.concatenate([trainDataX, xValues], axis=0)\n",
        "    trainDataY = np.concatenate([trainDataY, yValues], axis=0)\n",
        "\n",
        "    #if readPos % 50 == 0 and readPos > 0: #why readPos % 50 == 0? need 50 or 100 stocks?\n",
        "    if readPos > 0:\n",
        "        print(\"Loaded \" + str(readPos) + \" training files\")\n",
        "\n",
        "    readPos += 1\n",
        "print(\"\\n\\n\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 1 training files\n",
            "Loaded 2 training files\n",
            "Loaded 3 training files\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDWcfZqXIAtq",
        "outputId": "2cfea75b-46b1-4ab7-9de8-5cceb668aadc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Reads evalutation data into memory\n",
        "readPos = 0\n",
        "for fileName in evalFiles:\n",
        "    importedData = pandas.read_csv(\"./eval/\" + fileName, sep=',')\n",
        "\n",
        "    xValuesDF = importedData[[\"RSI14\", \"RSI50\", \"STOCH14K\", \"STOCH14D\"]]\n",
        "    yValuesDF = importedData[[\"longOutput\", \"shortOutput\"]]\n",
        "\n",
        "    xValues = np.array(xValuesDF.values.tolist())\n",
        "    yValues = np.array(yValuesDF.values.tolist())\n",
        "\n",
        "    evalDataX = np.concatenate([evalDataX, xValues], axis=0)\n",
        "    evalDataY = np.concatenate([evalDataY, yValues], axis=0)\n",
        "\n",
        "    #if readPos % 50 == 0 and readPos > 0:\n",
        "    if readPos > 0:\n",
        "        print(\"Loaded \" + str(readPos) + \" evaluating files\")\n",
        "\n",
        "    readPos += 1\n",
        "print(\"\\n\\n\")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 1 evaluating files\n",
            "Loaded 2 evaluating files\n",
            "Loaded 3 evaluating files\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyixXVSgx1Ry"
      },
      "source": [
        "tfc.logging.set_verbosity(tfc.logging.INFO)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POVUrKKqIL5n"
      },
      "source": [
        "# used to sample batches from your data for training\n",
        "def createTrainingBatch(amount):\n",
        "\n",
        "    randomBatchPos = np.random.randint(0, trainDataX.shape[0], amount)\n",
        "\n",
        "    xOut = trainDataX[randomBatchPos]\n",
        "    yOut = trainDataY[randomBatchPos]\n",
        "\n",
        "    return xOut, yOut"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHcOQN4tHlOm"
      },
      "source": [
        "# ML training and evaluation functions\n",
        "def train_tfc():\n",
        "    globalStepTensor = tfc.Variable(0, trainable=False, name='global_step')\n",
        "\n",
        "    sess = tfc.InteractiveSession()\n",
        "\n",
        "    # placeholder for the input features\n",
        "    x = tfc.placeholder(tfc.float32, [None, 4])\n",
        "    # placeholder for the one-hot labels\n",
        "    y = tfc.placeholder(tfc.float32, [None, 2])\n",
        "    # placeholder for node dropout rate\n",
        "    internalDropout = tfc.placeholder(tfc.float32, None)\n",
        "\n",
        "    net = x  # input layer is the trading indicators\n",
        "\n",
        "    # Creates the neural network model\n",
        "    with tfc.name_scope('network'):\n",
        "        # Initialises each layer in the network\n",
        "        layerPos = 0\n",
        "        for units in nodeLayout:\n",
        "            net = tfc.layers.dense(\n",
        "                net,\n",
        "                units=units,\n",
        "                activation=tfc.nn.tanh,\n",
        "                name=str(\n",
        "                    \"dense\" +\n",
        "                    str(units) +\n",
        "                    \"_\" +\n",
        "                    str(layerPos)))  # adds each layer to the networm as specified by nodeLayout\n",
        "            # dropout layer after each layer\n",
        "            net = tfc.layers.dropout(net, rate=internalDropout)\n",
        "            layerPos += 1\n",
        "\n",
        "    logits = tfc.layers.dense(\n",
        "        net, 2, activation=tfc.nn.softmax)  # network output\n",
        "\n",
        "    with tfc.name_scope('lossFunction'):\n",
        "        cross_entropy_loss = tfc.reduce_mean(\n",
        "            tfc.nn.softmax_cross_entropy_with_logits_v2(\n",
        "                labels=y,\n",
        "                logits=logits))  # on NO account put this within a name scope - tensorboard shits itself\n",
        "\n",
        "    with tfc.name_scope('trainingStep'):\n",
        "        tfc.summary.scalar('crossEntropyLoss', cross_entropy_loss)\n",
        "        trainStep = tfc.train.AdamOptimizer(0.0001).minimize(\n",
        "            cross_entropy_loss, global_step=globalStepTensor)\n",
        "\n",
        "    with tfc.name_scope('accuracy'):\n",
        "        correctPrediction = tfc.equal(tfc.argmax(logits, 1), tfc.argmax(y, 1))\n",
        "        accuracy = tfc.reduce_mean(tfc.cast(correctPrediction, tfc.float32))\n",
        "        tfc.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "    merged = tfc.summary.merge_all()\n",
        "    trainWriter = tfc.summary.FileWriter(\n",
        "        mainDirectory + '/train', sess.graph, flush_secs=1, max_queue=2)\n",
        "    evalWriter = tfc.summary.FileWriter(\n",
        "        mainDirectory + '/eval', sess.graph, flush_secs=1, max_queue=2)\n",
        "    tfc.global_variables_initializer().run()\n",
        "\n",
        "    # Saves the model at defined checkpoints and loads any available model at\n",
        "    # start-up\n",
        "    saver = tfc.train.Saver(max_to_keep=2, name=\"checkpoint\")\n",
        "    path = tfc.train.get_checkpoint_state(mainDirectory)\n",
        "    if path is not None:\n",
        "        saver.restore(sess, tfc.train.latest_checkpoint(mainDirectory))\n",
        "\n",
        "    lastTime = time()\n",
        "    while tfc.train.global_step(sess, globalStepTensor) <= trainingCycles:\n",
        "        globalStep = tfc.train.global_step(sess, globalStepTensor)\n",
        "\n",
        "        # generates batch for each training cycle\n",
        "        xFeed, yFeed = createTrainingBatch(batchSize)\n",
        "\n",
        "        # Record summaries and accuracy on both train and eval data\n",
        "        if globalStep % summarySteps == 0:\n",
        "            currentTime = time()\n",
        "            totalTime = (currentTime - lastTime)\n",
        "            print(str(totalTime) + \" seconds, \" +\n",
        "                  str(summarySteps / totalTime) + \" steps/sec\")\n",
        "            lastTime = currentTime\n",
        "\n",
        "            summary, accuracyOut, _ = sess.run([\n",
        "                merged,\n",
        "                accuracy,\n",
        "                trainStep\n",
        "            ],\n",
        "                feed_dict={\n",
        "                    x: xFeed,\n",
        "                    y: yFeed,\n",
        "                    internalDropout: dropout\n",
        "                })\n",
        "            trainWriter.add_summary(summary, globalStep)\n",
        "            trainWriter.flush()\n",
        "            print('Train accuracy at step %s: %s' % (globalStep, accuracyOut))\n",
        "\n",
        "            summary, accuracyOut = sess.run([\n",
        "                merged,\n",
        "                accuracy,\n",
        "            ],\n",
        "                feed_dict={\n",
        "                    x: evalDataX,\n",
        "                    y: evalDataY,\n",
        "                    internalDropout: 0\n",
        "                })\n",
        "            evalWriter.add_summary(summary, globalStep)\n",
        "            evalWriter.flush()\n",
        "            print('Eval accuracy at step %s: %s' % (globalStep, accuracyOut))\n",
        "\n",
        "            print(\"\\n\\n\")\n",
        "            saver.save(sess, save_path=str(mainDirectory + \"model\"),\n",
        "                       global_step=globalStep)  # saves a snapshot of the model\n",
        "\n",
        "        else:  # Training cycle\n",
        "            _ = sess.run(\n",
        "                [trainStep], feed_dict={\n",
        "                    x: xFeed,\n",
        "                    y: yFeed,\n",
        "                    internalDropout: dropout\n",
        "                })\n",
        "\n",
        "    trainWriter.close()\n",
        "    evalWriter.close()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p-Jg3ymzOWH"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcJVF-yjIH5w"
      },
      "source": [
        "tfc.disable_eager_execution()\n",
        "\n",
        "#execute training //accuracy not ideal\n",
        "train_tfc()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}