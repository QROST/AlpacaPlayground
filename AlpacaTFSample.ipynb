{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlpacaTFSample.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN3hcxU/hC0UVFWcoHglL9e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QROST/AlpacaPlayground/blob/main/AlpacaTFSample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t56BN2JXXn_K",
        "outputId": "d6dde6af-756c-4c58-d76d-111ef98a7d59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        }
      },
      "source": [
        "!pip install alpaca-trade-api\n",
        "!pip install --upgrade mplfinance\n",
        "!wget https://launchpad.net/~mario-mariomedina/+archive/ubuntu/talib/+files/libta-lib0_0.4.0-oneiric1_amd64.deb -qO libta.deb\n",
        "!wget https://launchpad.net/~mario-mariomedina/+archive/ubuntu/talib/+files/ta-lib0-dev_0.4.0-oneiric1_amd64.deb -qO ta.deb\n",
        "!dpkg -i libta.deb ta.deb\n",
        "!pip install ta-lib"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: alpaca-trade-api in /usr/local/lib/python3.6/dist-packages (0.51.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from alpaca-trade-api) (1.1.2)\n",
            "Requirement already satisfied: websockets<9,>=8.0 in /usr/local/lib/python3.6/dist-packages (from alpaca-trade-api) (8.1)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.6/dist-packages (from alpaca-trade-api) (2.23.0)\n",
            "Requirement already satisfied: websocket-client<1,>=0.56.0 in /usr/local/lib/python3.6/dist-packages (from alpaca-trade-api) (0.57.0)\n",
            "Requirement already satisfied: urllib3<1.26,>1.24 in /usr/local/lib/python3.6/dist-packages (from alpaca-trade-api) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->alpaca-trade-api) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas->alpaca-trade-api) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->alpaca-trade-api) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>2->alpaca-trade-api) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>2->alpaca-trade-api) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>2->alpaca-trade-api) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client<1,>=0.56.0->alpaca-trade-api) (1.15.0)\n",
            "Requirement already up-to-date: mplfinance in /usr/local/lib/python3.6/dist-packages (0.12.7a0)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from mplfinance) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from mplfinance) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mplfinance) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mplfinance) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mplfinance) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mplfinance) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mplfinance) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->mplfinance) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->mplfinance) (1.15.0)\n",
            "Selecting previously unselected package libta-lib0.\n",
            "(Reading database ... 144611 files and directories currently installed.)\n",
            "Preparing to unpack libta.deb ...\n",
            "Unpacking libta-lib0 (0.4.0-oneiric1) ...\n",
            "Selecting previously unselected package ta-lib0-dev.\n",
            "Preparing to unpack ta.deb ...\n",
            "Unpacking ta-lib0-dev (0.4.0-oneiric1) ...\n",
            "Setting up libta-lib0 (0.4.0-oneiric1) ...\n",
            "Setting up ta-lib0-dev (0.4.0-oneiric1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting ta-lib\n",
            "  Using cached https://files.pythonhosted.org/packages/ac/cf/681911aa31e04ba171ab4d523a412f4a746e30d3eacb1738799d181e028b/TA-Lib-0.4.19.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ta-lib) (1.18.5)\n",
            "Building wheels for collected packages: ta-lib\n",
            "  Building wheel for ta-lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta-lib: filename=TA_Lib-0.4.19-cp36-cp36m-linux_x86_64.whl size=1437799 sha256=a3b16cd6d9204cdb2b1f474b4d0259d34c142520783cc320b79e81bce5e432df\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/f6/12/3d1ccd06caadd8fa47e016991dd0d27f1163bb260f1854e2ff\n",
            "Successfully built ta-lib\n",
            "Installing collected packages: ta-lib\n",
            "Successfully installed ta-lib-0.4.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPhPBaw6XZ4L"
      },
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import talib\n",
        "import alpaca_trade_api as tradeapi\n",
        "import pandas\n",
        "from time import sleep\n",
        "import os\n",
        "\n",
        "\n",
        "# Creates dataset folders in directory script is run from\n",
        "try:\n",
        "    os.stat(\"./train\")\n",
        "    os.stat(\"./eval\")\n",
        "except BaseException:\n",
        "    os.mkdir(\"./train\")\n",
        "    os.mkdir(\"./eval\")\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i533XephXlPE",
        "outputId": "5b5943fd-7fec-48fc-d863-28a9f877285e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "# api = tradeapi.REST(key_id= < your key id >, secret_key= < your secret\n",
        "# key > )\n",
        "\n",
        "barTimeframe = \"1D\"  # 1Min, 5Min, 15Min, 1H, 1D\n",
        "\n",
        "print(os.path.isfile(\"/content/sample_data/assetList.txt\"))\n",
        "\n",
        "assetList = np.loadtxt(\n",
        "    \"/content/sample_data/assetList.txt\",\n",
        "    comments=\"#\",\n",
        "    delimiter=\",\",\n",
        "    unpack=False,\n",
        "    dtype=\"str\",\n",
        "    ndmin=2)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPAWmO_DbzPo"
      },
      "source": [
        "\n",
        "# ISO8601 date format\n",
        "trainStartDate = \"2015-01-01T00:00:00.000Z\"\n",
        "trainEndDate = \"2017-06-01T00:00:00.000Z\"\n",
        "evalStartDate = \"2017-06-01T00:00:00.000Z\"\n",
        "evalEndDate = \"2018-06-01T00:00:00.000Z\"\n",
        "\n",
        "targetLookaheadPeriod = 1\n",
        "startCutoffPeriod = 50  # Set to length of maximum period indicator\n",
        "\n",
        "\n",
        "# Tracks position in list of symbols to download\n",
        "iteratorPos = 0\n",
        "assetListLen = len(assetList)\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbmwFdjWcq3K",
        "outputId": "4c4d4d95-5b0c-4ced-91ad-ab5a20a394e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "\n",
        "while iteratorPos < assetListLen:\n",
        "    try:\n",
        "        symbol = assetList[iteratorPos]\n",
        "\n",
        "        # Returns market data as a pandas dataframe\n",
        "        returned_data = api.get_bars(\n",
        "            symbol,\n",
        "            barTimeframe,\n",
        "            start_dt=trainStartDate,\n",
        "            end_dt=evalEndDate).df\n",
        "\n",
        "        # Processes all data into numpy arrays for use by talib\n",
        "        timeList = np.array(returned_data.index)\n",
        "        openList = np.array(returned_data.open, dtype=np.float64)\n",
        "        highList = np.array(returned_data.high, dtype=np.float64)\n",
        "        lowList = np.array(returned_data.low, dtype=np.float64)\n",
        "        closeList = np.array(returned_data.close, dtype=np.float64)\n",
        "        volumeList = np.array(returned_data.volume, dtype=np.float64)\n",
        "\n",
        "        # Adjusts data lists due to the reward function look ahead period\n",
        "        shiftedTimeList = timeList[:-targetLookaheadPeriod]\n",
        "        shiftedClose = closeList[targetLookaheadPeriod:]\n",
        "        highList = highList[:-targetLookaheadPeriod]\n",
        "        lowList = lowList[:-targetLookaheadPeriod]\n",
        "        closeList = closeList[:-targetLookaheadPeriod]\n",
        "\n",
        "        # Calculate trading indicators\n",
        "        RSI14 = talib.RSI(closeList, 14)\n",
        "        RSI50 = talib.RSI(closeList, 50)\n",
        "        STOCH14K, STOCH14D = talib.STOCH(\n",
        "            highList, lowList, closeList, fastk_period=14, slowk_period=3, slowd_period=3)\n",
        "\n",
        "        # Calulate network target/ reward function for training\n",
        "        closeDifference = shiftedClose - closeList\n",
        "        closeDifferenceLen = len(closeDifference)\n",
        "\n",
        "        # Creates a binary output if the market moves up or down, for use as\n",
        "        # one-hot labels\n",
        "        longOutput = np.zeros(closeDifferenceLen)\n",
        "        longOutput[closeDifference >= 0] = 1\n",
        "        shortOutput = np.zeros(closeDifferenceLen)\n",
        "        shortOutput[closeDifference < 0] = 1\n",
        "\n",
        "        # Constructs the dataframe and writes to CSV file\n",
        "        outputDF = {\n",
        "            \"close\": closeList,  # Not to be included in network training, only for later analysis\n",
        "            \"RSI14\": RSI14,\n",
        "            \"RSI50\": RSI50,\n",
        "            \"STOCH14K\": STOCH14K,\n",
        "            \"STOCH14D\": STOCH14D,\n",
        "            \"longOutput\": longOutput,\n",
        "            \"shortOutput\": shortOutput\n",
        "        }\n",
        "        # Makes sure the dataframe columns don't get mixed up\n",
        "        columnOrder = [\"close\", \"RSI14\", \"RSI50\", \"STOCH14K\",\n",
        "                       \"STOCH14D\", \"longOutput\", \"shortOutput\"]\n",
        "        outputDF = pandas.DataFrame(\n",
        "            data=outputDF,\n",
        "            index=shiftedTimeList,\n",
        "            columns=columnOrder)[\n",
        "            startCutoffPeriod:]\n",
        "\n",
        "        # Splits data into training and evaluation sets\n",
        "        trainingDF = outputDF[outputDF.index < evalStartDate]\n",
        "        evalDF = outputDF[outputDF.index >= evalStartDate]\n",
        "\n",
        "        if (len(trainingDF) > 0 and len(evalDF) > 0):\n",
        "            print(\"writing \" + str(symbol) +\n",
        "                  \", data len: \" + str(len(closeList)))\n",
        "\n",
        "            trainingDF.to_csv(\"./train/\" + symbol + \".csv\", index_label=\"date\")\n",
        "            evalDF.to_csv(\"./eval/\" + symbol + \".csv\", index_label=\"date\")\n",
        "    except BaseException:\n",
        "      print(\"something wrong\")\n",
        "        pass\n",
        "\n",
        "    sleep(5)  # To avoid API rate limits\n",
        "    iteratorPos += 1"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-3c39bea16483>\"\u001b[0;36m, line \u001b[0;32m76\u001b[0m\n\u001b[0;31m    pass\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrHK82SRX9td"
      },
      "source": [
        "2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32TlwqCFX-rQ"
      },
      "source": [
        "import argparse\n",
        "import sys\n",
        "import tempfile\n",
        "from time import time\n",
        "import random\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "\n",
        "import pandas\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn import metrics\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuCgLuuvbluO"
      },
      "source": [
        "\n",
        "# model settings\n",
        "# Static seed to allow for reproducability between training runs\n",
        "tf.set_random_seed(12345)\n",
        "trainingCycles = 500000  # Number of training steps before ending\n",
        "batchSize = 1000  # Number of examples per training batch\n",
        "summarySteps = 1000  # Number of training steps between each summary\n",
        "dropout = 0.5  # Node dropout for training\n",
        "nodeLayout = [40, 30, 20, 10]  # Layout of nodes in each layer\n",
        "\n",
        "\n",
        "mainDirectory = str(\"./model_1/\")\n",
        "\n",
        "trainFiles = [f for f in listdir(\"./train/\") if isfile(join(\"./train/\", f))]\n",
        "evalFiles = [f for f in listdir(\"./eval/\") if isfile(join(\"./eval/\", f))]\n",
        "\n",
        "\n",
        "# Initialises data arrays\n",
        "trainDataX = np.empty([0, 4])\n",
        "trainDataY = np.empty([0, 2])\n",
        "evalDataX = np.empty([0, 4])\n",
        "evalDataY = np.empty([0, 2])\n",
        "\n",
        "\n",
        "# Reads training data into memory\n",
        "readPos = 0\n",
        "for fileName in trainFiles:\n",
        "    importedData = pandas.read_csv(\"./train/\" + fileName, sep=',')\n",
        "\n",
        "    xValuesDF = importedData[[\"RSI14\", \"RSI50\", \"STOCH14K\", \"STOCH14D\"]]\n",
        "    yValuesDF = importedData[[\"longOutput\", \"shortOutput\"]]\n",
        "\n",
        "    xValues = np.array(xValuesDF.values.tolist())\n",
        "    yValues = np.array(yValuesDF.values.tolist())\n",
        "\n",
        "    trainDataX = np.concatenate([trainDataX, xValues], axis=0)\n",
        "    trainDataY = np.concatenate([trainDataY, yValues], axis=0)\n",
        "\n",
        "    if readPos % 50 == 0 and readPos > 0:\n",
        "        print(\"Loaded \" + str(readPos) + \" training files\")\n",
        "\n",
        "    readPos += 1\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "# Reads evalutation data into memory\n",
        "readPos = 0\n",
        "for fileName in evalFiles:\n",
        "    importedData = pandas.read_csv(\"./eval/\" + fileName, sep=',')\n",
        "\n",
        "    xValuesDF = importedData[[\"RSI14\", \"RSI50\", \"STOCH14K\", \"STOCH14D\"]]\n",
        "    yValuesDF = importedData[[\"longOutput\", \"shortOutput\"]]\n",
        "\n",
        "    xValues = np.array(xValuesDF.values.tolist())\n",
        "    yValues = np.array(yValuesDF.values.tolist())\n",
        "\n",
        "    evalDataX = np.concatenate([evalDataX, xValues], axis=0)\n",
        "    evalDataY = np.concatenate([evalDataY, yValues], axis=0)\n",
        "\n",
        "    if readPos % 50 == 0 and readPos > 0:\n",
        "        print(\"Loaded \" + str(readPos) + \" training files\")\n",
        "\n",
        "    readPos += 1\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "# used to sample batches from your data for training\n",
        "def createTrainingBatch(amount):\n",
        "\n",
        "    randomBatchPos = np.random.randint(0, trainDataX.shape[0], amount)\n",
        "\n",
        "    xOut = trainDataX[randomBatchPos]\n",
        "    yOut = trainDataY[randomBatchPos]\n",
        "\n",
        "    return xOut, yOut\n",
        "\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "# ML training and evaluation functions\n",
        "def train():\n",
        "    globalStepTensor = tf.Variable(0, trainable=False, name='global_step')\n",
        "\n",
        "    sess = tf.InteractiveSession()\n",
        "\n",
        "    # placeholder for the input features\n",
        "    x = tf.placeholder(tf.float32, [None, 4])\n",
        "    # placeholder for the one-hot labels\n",
        "    y = tf.placeholder(tf.float32, [None, 2])\n",
        "    # placeholder for node dropout rate\n",
        "    internalDropout = tf.placeholder(tf.float32, None)\n",
        "\n",
        "    net = x  # input layer is the trading indicators\n",
        "\n",
        "    # Creates the neural network model\n",
        "    with tf.name_scope('network'):\n",
        "        # Initialises each layer in the network\n",
        "        layerPos = 0\n",
        "        for units in nodeLayout:\n",
        "            net = tf.layers.dense(\n",
        "                net,\n",
        "                units=units,\n",
        "                activation=tf.nn.tanh,\n",
        "                name=str(\n",
        "                    \"dense\" +\n",
        "                    str(units) +\n",
        "                    \"_\" +\n",
        "                    str(layerPos)))  # adds each layer to the networm as specified by nodeLayout\n",
        "            # dropout layer after each layer\n",
        "            net = tf.layers.dropout(net, rate=internalDropout)\n",
        "            layerPos += 1\n",
        "\n",
        "    logits = tf.layers.dense(\n",
        "        net, 2, activation=tf.nn.softmax)  # network output\n",
        "\n",
        "    with tf.name_scope('lossFunction'):\n",
        "        cross_entropy_loss = tf.reduce_mean(\n",
        "            tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "                labels=y,\n",
        "                logits=logits))  # on NO account put this within a name scope - tensorboard shits itself\n",
        "\n",
        "    with tf.name_scope('trainingStep'):\n",
        "        tf.summary.scalar('crossEntropyLoss', cross_entropy_loss)\n",
        "        trainStep = tf.train.AdamOptimizer(0.0001).minimize(\n",
        "            cross_entropy_loss, global_step=globalStepTensor)\n",
        "\n",
        "    with tf.name_scope('accuracy'):\n",
        "        correctPrediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correctPrediction, tf.float32))\n",
        "        tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "    merged = tf.summary.merge_all()\n",
        "    trainWriter = tf.summary.FileWriter(\n",
        "        mainDirectory + '/train', sess.graph, flush_secs=1, max_queue=2)\n",
        "    evalWriter = tf.summary.FileWriter(\n",
        "        mainDirectory + '/eval', sess.graph, flush_secs=1, max_queue=2)\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "    # Saves the model at defined checkpoints and loads any available model at\n",
        "    # start-up\n",
        "    saver = tf.train.Saver(max_to_keep=2, name=\"checkpoint\")\n",
        "    path = tf.train.get_checkpoint_state(mainDirectory)\n",
        "    if path is not None:\n",
        "        saver.restore(sess, tf.train.latest_checkpoint(mainDirectory))\n",
        "\n",
        "    lastTime = time()\n",
        "    while tf.train.global_step(sess, globalStepTensor) <= trainingCycles:\n",
        "        globalStep = tf.train.global_step(sess, globalStepTensor)\n",
        "\n",
        "        # generates batch for each training cycle\n",
        "        xFeed, yFeed = createTrainingBatch(batchSize)\n",
        "\n",
        "        # Record summaries and accuracy on both train and eval data\n",
        "        if globalStep % summarySteps == 0:\n",
        "            currentTime = time()\n",
        "            totalTime = (currentTime - lastTime)\n",
        "            print(str(totalTime) + \" seconds, \" +\n",
        "                  str(summarySteps / totalTime) + \" steps/sec\")\n",
        "            lastTime = currentTime\n",
        "\n",
        "            summary, accuracyOut, _ = sess.run([\n",
        "                merged,\n",
        "                accuracy,\n",
        "                trainStep\n",
        "            ],\n",
        "                feed_dict={\n",
        "                    x: xFeed,\n",
        "                    y: yFeed,\n",
        "                    internalDropout: dropout\n",
        "                })\n",
        "            trainWriter.add_summary(summary, globalStep)\n",
        "            trainWriter.flush()\n",
        "            print('Train accuracy at step %s: %s' % (globalStep, accuracyOut))\n",
        "\n",
        "            summary, accuracyOut = sess.run([\n",
        "                merged,\n",
        "                accuracy,\n",
        "            ],\n",
        "                feed_dict={\n",
        "                    x: evalDataX,\n",
        "                    y: evalDataY,\n",
        "                    internalDropout: 0\n",
        "                })\n",
        "            evalWriter.add_summary(summary, globalStep)\n",
        "            evalWriter.flush()\n",
        "            print('Eval accuracy at step %s: %s' % (globalStep, accuracyOut))\n",
        "\n",
        "            print(\"\\n\\n\")\n",
        "            saver.save(sess, save_path=str(mainDirectory + \"model\"),\n",
        "                       global_step=globalStep)  # saves a snapshot of the model\n",
        "\n",
        "        else:  # Training cycle\n",
        "            _ = sess.run(\n",
        "                [trainStep], feed_dict={\n",
        "                    x: xFeed,\n",
        "                    y: yFeed,\n",
        "                    internalDropout: dropout\n",
        "                })\n",
        "\n",
        "    trainWriter.close()\n",
        "    evalWriter.close()\n",
        "\n",
        "\n",
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}