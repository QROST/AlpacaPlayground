{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlpacaTFSample.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyN2tM60sUcxf/0ovdA0R9OO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QROST/AlpacaPlayground/blob/main/AlpacaTFSample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t56BN2JXXn_K",
        "outputId": "7318e5d9-273c-4597-91a2-d88e520c236e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install alpaca-trade-api\n",
        "!pip install --upgrade mplfinance\n",
        "!wget https://launchpad.net/~mario-mariomedina/+archive/ubuntu/talib/+files/libta-lib0_0.4.0-oneiric1_amd64.deb -qO libta.deb\n",
        "!wget https://launchpad.net/~mario-mariomedina/+archive/ubuntu/talib/+files/ta-lib0-dev_0.4.0-oneiric1_amd64.deb -qO ta.deb\n",
        "!dpkg -i libta.deb ta.deb\n",
        "!pip install ta-lib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting alpaca-trade-api\n",
            "  Downloading https://files.pythonhosted.org/packages/cf/01/ee98cac996038ea7ec5b53dfa8cd8d681d92ee6e2d9f878f8f29e573f5ed/alpaca_trade_api-0.51.0-py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from alpaca-trade-api) (1.1.2)\n",
            "Requirement already satisfied: urllib3<1.26,>1.24 in /usr/local/lib/python3.6/dist-packages (from alpaca-trade-api) (1.24.3)\n",
            "Collecting websockets<9,>=8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.9MB/s \n",
            "\u001b[?25hCollecting websocket-client<1,>=0.56.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>2 in /usr/local/lib/python3.6/dist-packages (from alpaca-trade-api) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->alpaca-trade-api) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->alpaca-trade-api) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas->alpaca-trade-api) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client<1,>=0.56.0->alpaca-trade-api) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>2->alpaca-trade-api) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>2->alpaca-trade-api) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>2->alpaca-trade-api) (2020.6.20)\n",
            "Installing collected packages: websockets, websocket-client, alpaca-trade-api\n",
            "Successfully installed alpaca-trade-api-0.51.0 websocket-client-0.57.0 websockets-8.1\n",
            "Collecting mplfinance\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/01/3418bb0c9952d4a3c24893e883df8e39065d7a13e7d60ae4f139a9eafc78/mplfinance-0.12.7a0-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 1.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from mplfinance) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from mplfinance) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->mplfinance) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas->mplfinance) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->mplfinance) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mplfinance) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mplfinance) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mplfinance) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->mplfinance) (1.15.0)\n",
            "Installing collected packages: mplfinance\n",
            "Successfully installed mplfinance-0.12.7a0\n",
            "Selecting previously unselected package libta-lib0.\n",
            "(Reading database ... 144611 files and directories currently installed.)\n",
            "Preparing to unpack libta.deb ...\n",
            "Unpacking libta-lib0 (0.4.0-oneiric1) ...\n",
            "Selecting previously unselected package ta-lib0-dev.\n",
            "Preparing to unpack ta.deb ...\n",
            "Unpacking ta-lib0-dev (0.4.0-oneiric1) ...\n",
            "Setting up libta-lib0 (0.4.0-oneiric1) ...\n",
            "Setting up ta-lib0-dev (0.4.0-oneiric1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting ta-lib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/cf/681911aa31e04ba171ab4d523a412f4a746e30d3eacb1738799d181e028b/TA-Lib-0.4.19.tar.gz (267kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ta-lib) (1.18.5)\n",
            "Building wheels for collected packages: ta-lib\n",
            "  Building wheel for ta-lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta-lib: filename=TA_Lib-0.4.19-cp36-cp36m-linux_x86_64.whl size=1437793 sha256=47e194b7d794f73c191e987d629bb4c61f299b20c2ace815c746c1a7988c040d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/f6/12/3d1ccd06caadd8fa47e016991dd0d27f1163bb260f1854e2ff\n",
            "Successfully built ta-lib\n",
            "Installing collected packages: ta-lib\n",
            "Successfully installed ta-lib-0.4.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPhPBaw6XZ4L"
      },
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import talib\n",
        "import alpaca_trade_api as tradeapi\n",
        "import pandas\n",
        "from time import sleep\n",
        "import os\n",
        "\n",
        "\n",
        "# Creates dataset folders in directory script is run from\n",
        "try:\n",
        "    os.stat(\"./train\")\n",
        "    os.stat(\"./eval\")\n",
        "except BaseException:\n",
        "    os.mkdir(\"./train\")\n",
        "    os.mkdir(\"./eval\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbxW6voS1xMh"
      },
      "source": [
        "api = tradeapi.REST(\n",
        "    \"PKY2N25P0N57TF83ZKXS\",\n",
        "    \"wycEE2hPm0C2OOzyigVvikT4IZEboOMqTeDKOQhT\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i533XephXlPE"
      },
      "source": [
        "\n",
        "# api = tradeapi.REST(key_id= < your key id >, secret_key= < your secret\n",
        "# key > )\n",
        "\n",
        "barTimeframe = \"1D\"  # 1Min, 5Min, 15Min, 1H, 1D\n",
        "\n",
        "assetList = ['NVDA','MSFT']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPAWmO_DbzPo"
      },
      "source": [
        "\n",
        "# ISO8601 date format\n",
        "trainStartDate = \"2015-01-01T00:00:00.000Z\"\n",
        "trainEndDate = \"2017-06-01T00:00:00.000Z\"\n",
        "evalStartDate = \"2017-06-01T00:00:00.000Z\"\n",
        "evalEndDate = \"2020-06-01T00:00:00.000Z\"\n",
        "\n",
        "targetLookaheadPeriod = 1\n",
        "startCutoffPeriod = 50  # Set to length of maximum period indicator\n",
        "\n",
        "\n",
        "# Tracks position in list of symbols to download\n",
        "iteratorPos = 0\n",
        "assetListLen = len(assetList)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq75DnO02oZ0",
        "outputId": "7a248996-5575-49aa-fc03-613ce614dab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "symbol = assetList[iteratorPos]\n",
        "\n",
        "# Returns market data as a pandas dataframe\n",
        "returned_data = api.get_barset(\n",
        "    symbol,\n",
        "    barTimeframe,\n",
        "    start=trainStartDate,\n",
        "    end=evalEndDate).df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method NDFrame.first of                              NVDA                                    \n",
            "                             open    high      low    close    volume\n",
            "time                                                                 \n",
            "2014-12-31 00:00:00-05:00   20.40   20.51   19.990   20.050   3487614\n",
            "2015-01-02 00:00:00-05:00   20.13   20.28   19.811   20.125   2416481\n",
            "2015-01-05 00:00:00-05:00   20.13   20.19   19.700   19.800   3720891\n",
            "2015-01-06 00:00:00-05:00   19.82   19.84   19.170   19.180   4393047\n",
            "2015-01-07 00:00:00-05:00   19.33   19.50   19.080   19.140   5829847\n",
            "...                           ...     ...      ...      ...       ...\n",
            "2020-05-22 00:00:00-04:00  353.01  363.72  348.530  361.060  24510192\n",
            "2020-05-26 00:00:00-04:00  366.27  367.27  346.880  348.780  17690647\n",
            "2020-05-27 00:00:00-04:00  345.00  345.31  319.870  341.130  28411380\n",
            "2020-05-28 00:00:00-04:00  336.49  350.42  335.170  339.310  35425530\n",
            "2020-05-29 00:00:00-04:00  342.18  354.67  339.400  353.650  15763288\n",
            "\n",
            "[1362 rows x 5 columns]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-jrJlxw3g9E",
        "outputId": "46302312-9c3c-4891-b7f8-08bbabef1d33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Processes all data into numpy arrays for use by talib\n",
        "timeList = np.array(returned_data.index)\n",
        "openList = np.array(returned_data[symbol]['open'].tolist(), dtype=np.float64)\n",
        "print(openList)\n",
        "#print(returned_data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 20.4   20.13  20.13 ... 345.   336.49 342.18]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rwHe5v14xa9"
      },
      "source": [
        "highList = np.array(returned_data[symbol]['high'].tolist(), dtype=np.float64)\n",
        "lowList = np.array(returned_data[symbol]['low'].tolist(), dtype=np.float64)\n",
        "closeList = np.array(returned_data[symbol]['close'].tolist(), dtype=np.float64)\n",
        "volumeList = np.array(returned_data[symbol]['volume'].tolist(), dtype=np.float64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbmwFdjWcq3K"
      },
      "source": [
        "# Adjusts data lists due to the reward function look ahead period\n",
        "shiftedTimeList = timeList[:-targetLookaheadPeriod]\n",
        "shiftedClose = closeList[targetLookaheadPeriod:]\n",
        "highList = highList[:-targetLookaheadPeriod]\n",
        "lowList = lowList[:-targetLookaheadPeriod]\n",
        "closeList = closeList[:-targetLookaheadPeriod]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FElFOiVd3wXk"
      },
      "source": [
        "# Calculate trading indicators\n",
        "RSI14 = talib.RSI(closeList, 14)\n",
        "RSI50 = talib.RSI(closeList, 50)\n",
        "STOCH14K, STOCH14D = talib.STOCH(\n",
        "    highList, lowList, closeList, fastk_period=14, slowk_period=3, slowd_period=3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwHo14_xCQkQ"
      },
      "source": [
        "# Calulate network target/ reward function for training\n",
        "closeDifference = shiftedClose - closeList\n",
        "closeDifferenceLen = len(closeDifference)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWFCntyVCg_9"
      },
      "source": [
        "\n",
        "# Creates a binary output if the market moves up or down, for use as\n",
        "# one-hot labels\n",
        "longOutput = np.zeros(closeDifferenceLen)\n",
        "longOutput[closeDifference >= 0] = 1\n",
        "shortOutput = np.zeros(closeDifferenceLen)\n",
        "shortOutput[closeDifference < 0] = 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhELuR4dCjPG"
      },
      "source": [
        "\n",
        "# Constructs the dataframe and writes to CSV file\n",
        "outputDF = {\n",
        "    \"close\": closeList,  # Not to be included in network training, only for later analysis\n",
        "    \"RSI14\": RSI14,\n",
        "    \"RSI50\": RSI50,\n",
        "    \"STOCH14K\": STOCH14K,\n",
        "    \"STOCH14D\": STOCH14D,\n",
        "    \"longOutput\": longOutput,\n",
        "    \"shortOutput\": shortOutput\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8_TEcYkClX-",
        "outputId": "937bd0f6-08b2-45d5-e301-bb71d8281a52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "\n",
        "# Makes sure the dataframe columns don't get mixed up\n",
        "columnOrder = [\"close\", \"RSI14\", \"RSI50\", \"STOCH14K\",\n",
        "                \"STOCH14D\", \"longOutput\", \"shortOutput\"]\n",
        "outputDF = pandas.DataFrame(\n",
        "    data=outputDF,\n",
        "    index=shiftedTimeList,\n",
        "    columns=columnOrder)[\n",
        "    startCutoffPeriod:]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mform_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1684\u001b[0;31m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1360, 7), indices imply (1361, 7)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-107-591d4407e0d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputDF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshiftedTimeList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumnOrder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     startCutoffPeriod:]\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         ]\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1360, 7), indices imply (1361, 7)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6Xvqj4bC5d8"
      },
      "source": [
        "\n",
        "# Splits data into training and evaluation sets\n",
        "trainingDF = outputDF[outputDF.index < evalStartDate]\n",
        "evalDF = outputDF[outputDF.index >= evalStartDate]\n",
        "\n",
        "if (len(trainingDF) > 0 and len(evalDF) > 0):\n",
        "    print(\"writing \" + str(symbol) +\n",
        "          \", data len: \" + str(len(closeList)))\n",
        "\n",
        "    trainingDF.to_csv(\"./train/\" + symbol + \".csv\", index_label=\"date\")\n",
        "    evalDF.to_csv(\"./eval/\" + symbol + \".csv\", index_label=\"date\")    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYurm50x2fpD"
      },
      "source": [
        "# Splits data into training and evaluation sets\n",
        "        trainingDF = outputDF[outputDF.index < evalStartDate]\n",
        "        evalDF = outputDF[outputDF.index >= evalStartDate]\n",
        "\n",
        "        if (len(trainingDF) > 0 and len(evalDF) > 0):\n",
        "            print(\"writing \" + str(symbol) +\n",
        "                  \", data len: \" + str(len(closeList)))\n",
        "\n",
        "            trainingDF.to_csv(\"./train/\" + symbol + \".csv\", index_label=\"date\")\n",
        "            evalDF.to_csv(\"./eval/\" + symbol + \".csv\", index_label=\"date\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrHK82SRX9td"
      },
      "source": [
        "2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32TlwqCFX-rQ"
      },
      "source": [
        "import argparse\n",
        "import sys\n",
        "import tempfile\n",
        "from time import time\n",
        "import random\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "\n",
        "import pandas\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn import metrics\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuCgLuuvbluO"
      },
      "source": [
        "\n",
        "# model settings\n",
        "# Static seed to allow for reproducability between training runs\n",
        "tf.set_random_seed(12345)\n",
        "trainingCycles = 500000  # Number of training steps before ending\n",
        "batchSize = 1000  # Number of examples per training batch\n",
        "summarySteps = 1000  # Number of training steps between each summary\n",
        "dropout = 0.5  # Node dropout for training\n",
        "nodeLayout = [40, 30, 20, 10]  # Layout of nodes in each layer\n",
        "\n",
        "\n",
        "mainDirectory = str(\"./model_1/\")\n",
        "\n",
        "trainFiles = [f for f in listdir(\"./train/\") if isfile(join(\"./train/\", f))]\n",
        "evalFiles = [f for f in listdir(\"./eval/\") if isfile(join(\"./eval/\", f))]\n",
        "\n",
        "\n",
        "# Initialises data arrays\n",
        "trainDataX = np.empty([0, 4])\n",
        "trainDataY = np.empty([0, 2])\n",
        "evalDataX = np.empty([0, 4])\n",
        "evalDataY = np.empty([0, 2])\n",
        "\n",
        "\n",
        "# Reads training data into memory\n",
        "readPos = 0\n",
        "for fileName in trainFiles:\n",
        "    importedData = pandas.read_csv(\"./train/\" + fileName, sep=',')\n",
        "\n",
        "    xValuesDF = importedData[[\"RSI14\", \"RSI50\", \"STOCH14K\", \"STOCH14D\"]]\n",
        "    yValuesDF = importedData[[\"longOutput\", \"shortOutput\"]]\n",
        "\n",
        "    xValues = np.array(xValuesDF.values.tolist())\n",
        "    yValues = np.array(yValuesDF.values.tolist())\n",
        "\n",
        "    trainDataX = np.concatenate([trainDataX, xValues], axis=0)\n",
        "    trainDataY = np.concatenate([trainDataY, yValues], axis=0)\n",
        "\n",
        "    if readPos % 50 == 0 and readPos > 0:\n",
        "        print(\"Loaded \" + str(readPos) + \" training files\")\n",
        "\n",
        "    readPos += 1\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "# Reads evalutation data into memory\n",
        "readPos = 0\n",
        "for fileName in evalFiles:\n",
        "    importedData = pandas.read_csv(\"./eval/\" + fileName, sep=',')\n",
        "\n",
        "    xValuesDF = importedData[[\"RSI14\", \"RSI50\", \"STOCH14K\", \"STOCH14D\"]]\n",
        "    yValuesDF = importedData[[\"longOutput\", \"shortOutput\"]]\n",
        "\n",
        "    xValues = np.array(xValuesDF.values.tolist())\n",
        "    yValues = np.array(yValuesDF.values.tolist())\n",
        "\n",
        "    evalDataX = np.concatenate([evalDataX, xValues], axis=0)\n",
        "    evalDataY = np.concatenate([evalDataY, yValues], axis=0)\n",
        "\n",
        "    if readPos % 50 == 0 and readPos > 0:\n",
        "        print(\"Loaded \" + str(readPos) + \" training files\")\n",
        "\n",
        "    readPos += 1\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "# used to sample batches from your data for training\n",
        "def createTrainingBatch(amount):\n",
        "\n",
        "    randomBatchPos = np.random.randint(0, trainDataX.shape[0], amount)\n",
        "\n",
        "    xOut = trainDataX[randomBatchPos]\n",
        "    yOut = trainDataY[randomBatchPos]\n",
        "\n",
        "    return xOut, yOut\n",
        "\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "# ML training and evaluation functions\n",
        "def train():\n",
        "    globalStepTensor = tf.Variable(0, trainable=False, name='global_step')\n",
        "\n",
        "    sess = tf.InteractiveSession()\n",
        "\n",
        "    # placeholder for the input features\n",
        "    x = tf.placeholder(tf.float32, [None, 4])\n",
        "    # placeholder for the one-hot labels\n",
        "    y = tf.placeholder(tf.float32, [None, 2])\n",
        "    # placeholder for node dropout rate\n",
        "    internalDropout = tf.placeholder(tf.float32, None)\n",
        "\n",
        "    net = x  # input layer is the trading indicators\n",
        "\n",
        "    # Creates the neural network model\n",
        "    with tf.name_scope('network'):\n",
        "        # Initialises each layer in the network\n",
        "        layerPos = 0\n",
        "        for units in nodeLayout:\n",
        "            net = tf.layers.dense(\n",
        "                net,\n",
        "                units=units,\n",
        "                activation=tf.nn.tanh,\n",
        "                name=str(\n",
        "                    \"dense\" +\n",
        "                    str(units) +\n",
        "                    \"_\" +\n",
        "                    str(layerPos)))  # adds each layer to the networm as specified by nodeLayout\n",
        "            # dropout layer after each layer\n",
        "            net = tf.layers.dropout(net, rate=internalDropout)\n",
        "            layerPos += 1\n",
        "\n",
        "    logits = tf.layers.dense(\n",
        "        net, 2, activation=tf.nn.softmax)  # network output\n",
        "\n",
        "    with tf.name_scope('lossFunction'):\n",
        "        cross_entropy_loss = tf.reduce_mean(\n",
        "            tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "                labels=y,\n",
        "                logits=logits))  # on NO account put this within a name scope - tensorboard shits itself\n",
        "\n",
        "    with tf.name_scope('trainingStep'):\n",
        "        tf.summary.scalar('crossEntropyLoss', cross_entropy_loss)\n",
        "        trainStep = tf.train.AdamOptimizer(0.0001).minimize(\n",
        "            cross_entropy_loss, global_step=globalStepTensor)\n",
        "\n",
        "    with tf.name_scope('accuracy'):\n",
        "        correctPrediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correctPrediction, tf.float32))\n",
        "        tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "    merged = tf.summary.merge_all()\n",
        "    trainWriter = tf.summary.FileWriter(\n",
        "        mainDirectory + '/train', sess.graph, flush_secs=1, max_queue=2)\n",
        "    evalWriter = tf.summary.FileWriter(\n",
        "        mainDirectory + '/eval', sess.graph, flush_secs=1, max_queue=2)\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "    # Saves the model at defined checkpoints and loads any available model at\n",
        "    # start-up\n",
        "    saver = tf.train.Saver(max_to_keep=2, name=\"checkpoint\")\n",
        "    path = tf.train.get_checkpoint_state(mainDirectory)\n",
        "    if path is not None:\n",
        "        saver.restore(sess, tf.train.latest_checkpoint(mainDirectory))\n",
        "\n",
        "    lastTime = time()\n",
        "    while tf.train.global_step(sess, globalStepTensor) <= trainingCycles:\n",
        "        globalStep = tf.train.global_step(sess, globalStepTensor)\n",
        "\n",
        "        # generates batch for each training cycle\n",
        "        xFeed, yFeed = createTrainingBatch(batchSize)\n",
        "\n",
        "        # Record summaries and accuracy on both train and eval data\n",
        "        if globalStep % summarySteps == 0:\n",
        "            currentTime = time()\n",
        "            totalTime = (currentTime - lastTime)\n",
        "            print(str(totalTime) + \" seconds, \" +\n",
        "                  str(summarySteps / totalTime) + \" steps/sec\")\n",
        "            lastTime = currentTime\n",
        "\n",
        "            summary, accuracyOut, _ = sess.run([\n",
        "                merged,\n",
        "                accuracy,\n",
        "                trainStep\n",
        "            ],\n",
        "                feed_dict={\n",
        "                    x: xFeed,\n",
        "                    y: yFeed,\n",
        "                    internalDropout: dropout\n",
        "                })\n",
        "            trainWriter.add_summary(summary, globalStep)\n",
        "            trainWriter.flush()\n",
        "            print('Train accuracy at step %s: %s' % (globalStep, accuracyOut))\n",
        "\n",
        "            summary, accuracyOut = sess.run([\n",
        "                merged,\n",
        "                accuracy,\n",
        "            ],\n",
        "                feed_dict={\n",
        "                    x: evalDataX,\n",
        "                    y: evalDataY,\n",
        "                    internalDropout: 0\n",
        "                })\n",
        "            evalWriter.add_summary(summary, globalStep)\n",
        "            evalWriter.flush()\n",
        "            print('Eval accuracy at step %s: %s' % (globalStep, accuracyOut))\n",
        "\n",
        "            print(\"\\n\\n\")\n",
        "            saver.save(sess, save_path=str(mainDirectory + \"model\"),\n",
        "                       global_step=globalStep)  # saves a snapshot of the model\n",
        "\n",
        "        else:  # Training cycle\n",
        "            _ = sess.run(\n",
        "                [trainStep], feed_dict={\n",
        "                    x: xFeed,\n",
        "                    y: yFeed,\n",
        "                    internalDropout: dropout\n",
        "                })\n",
        "\n",
        "    trainWriter.close()\n",
        "    evalWriter.close()\n",
        "\n",
        "\n",
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}